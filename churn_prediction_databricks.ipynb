{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Projeto: Churn Prediction (SaaS 2025)\n",
    "**Engenharia de Dados e Machine Learning no Databricks**\n",
    "\n",
    "Este notebook executa o pipeline completo da camada **Silver** para a **Gold**, treinando um modelo de Random Forest para prever a probabilidade de cancelamento de assinaturas baseado no comportamento do usu√°rio e no sentimento extra√≠do (VoC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Ingest√£o (Silver Layer)\n",
    "Carregamos os dados brutos e realizamos a tipagem correta das colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Caminho definido para o Volume do Databricks\n",
    "path_silver = \"/Volumes/workspace/voc/churn/churn_silver_2025.csv\"\n",
    "\n",
    "df_silver = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(path_silver)\n",
    "\n",
    "display(df_silver.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering (Gold Layer)\n",
    "Aqui transformamos dados brutos em intelig√™ncia de neg√≥cio:\n",
    "- **Uso relativo:** Efici√™ncia do uso do app em rela√ß√£o ao valor pago.\n",
    "- **Tempo de Contrato:** Quantidade de dias desde a assinatura at√© a data de hoje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = df_silver.withColumn(\"data_assinatura\", F.to_date(\"data_assinatura\")) \\\n",
    "    .withColumn(\"uso_por_valor\", F.col(\"total_logs_app_30d\") / F.col(\"valor_mensalidade\")) \\\n",
    "    .withColumn(\"dias_ativo\", F.datediff(F.current_date(), F.col(\"data_assinatura\"))) \\\n",
    "    .fillna(0)\n",
    "\n",
    "print(\"Camada Gold gerada com sucesso!\")\n",
    "display(df_gold.select(\"id_cliente\", \"uso_por_valor\", \"dias_ativo\", \"churn\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o dos Dados para Machine Learning\n",
    "Convertemos categorias de texto em n√∫meros e agrupamos as features em um vetor denso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Indexando categorias de texto\n",
    "indexer = StringIndexer(inputCol=\"categoria_principal_voc\", outputCol=\"categoria_index\")\n",
    "\n",
    "# Definindo as colunas preditoras\n",
    "feature_cols = [\n",
    "    \"valor_mensalidade\", \n",
    "    \"total_logs_app_30d\", \n",
    "    \"tickets_suporte_abertos\", \n",
    "    \"score_sentimento_voc\", \n",
    "    \"uso_por_valor\", \n",
    "    \"dias_ativo\", \n",
    "    \"categoria_index\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do Modelo (Random Forest)\n",
    "Dividimos os dados (80% treino / 20% teste) e aplicamos o algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "train_data, test_data = df_gold.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"churn\", featuresCol=\"features\", numTrees=100)\n",
    "\n",
    "# Criando o Pipeline completo\n",
    "pipeline = Pipeline(stages=[indexer, assembler, rf])\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "print(\"Modelo treinado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Avalia√ß√£o de Performance\n",
    "Analisamos a capacidade do modelo de distinguir entre clientes que ficam e que saem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "eval_auc = BinaryClassificationEvaluator(labelCol=\"churn\", metricName=\"areaUnderROC\")\n",
    "auc = eval_auc.evaluate(predictions)\n",
    "\n",
    "eval_acc = MulticlassClassificationEvaluator(labelCol=\"churn\", metricName=\"accuracy\")\n",
    "accuracy = eval_acc.evaluate(predictions)\n",
    "\n",
    "print(f\"Resultados: AUC-ROC: {auc:.4f} | Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exporta√ß√£o para o Looker (Gold Layer Final)\n",
    "Extra√≠mos a probabilidade (que vem no vetor `probability` do Spark) para uma coluna leg√≠vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "churn_predictions_gold = predictions.withColumn(\"probabilidade_churn\", vector_to_array(\"probability\")[1]) \\\n",
    "    .select(\"id_cliente\", \"churn\", \"prediction\", \"probabilidade_churn\", \"score_sentimento_voc\")\n",
    "\n",
    "# Salvando como Delta Table (Ideal para o Looker ler via Databricks SQL)\n",
    "# churn_predictions_gold.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_churn_predictions\")\n",
    "\n",
    "display(churn_predictions_gold.orderBy(F.desc(\"probabilidade_churn\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
