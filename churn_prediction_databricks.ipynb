{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction Pipeline\n",
    "Este notebook implementa um pipeline completo de previsão de churn usando PySpark no Databricks.\n",
    "\n",
    "## Estrutura:\n",
    "- Ingestão (Silver Layer)\n",
    "- Transformação e Feature Engineering (Gold Layer)\n",
    "- Treinamento de Modelo (Random Forest)\n",
    "- Avaliação (Acurácia e AUC-ROC)\n",
    "- Saída Final (churn_predictions_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingestão de Dados (Silver Layer)\n",
    "Carregamos os dados da camada Silver a partir do arquivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ChurnPredictionPipeline\").getOrCreate()\n",
    "\n",
    "# Caminho do dataset Silver\n",
    "silver_path = \"/Volumes/workspace/voc/churn/churn_silver_2025.csv\"\n",
    "\n",
    "df_silver = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(silver_path)\n",
    "df_silver.printSchema()\n",
    "df_silver.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformação e Feature Engineering (Gold Layer)\n",
    "Criamos novas features, como a taxa de uso por valor pago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: taxa de uso por valor pago\n",
    "df_gold = df_silver.withColumn(\"taxa_uso_valor\", col(\"total_logs_app_30d\") / col(\"valor_mensalidade\"))\n",
    "\n",
    "# Conversão da coluna alvo para inteiro\n",
    "df_gold = df_gold.withColumn(\"label\", col(\"churn\").cast(\"integer\"))\n",
    "df_gold.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparação dos Dados para Machine Learning\n",
    "Usamos StringIndexer para variáveis categóricas e VectorAssembler para juntar todas as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Indexação da variável categórica\n",
    "indexer = StringIndexer(inputCol=\"categoria_principal_voc\", outputCol=\"categoria_index\")\n",
    "\n",
    "# Seleção de features\n",
    "feature_cols = [\"valor_mensalidade\", \"total_logs_app_30d\", \"tickets_suporte_abertos\", \"score_sentimento_voc\", \"taxa_uso_valor\", \"categoria_index\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, assembler])\n",
    "df_ml = pipeline.fit(df_gold).transform(df_gold)\n",
    "df_ml.select(\"id_cliente\", \"features\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Treinamento do Modelo (Random Forest)\n",
    "Dividimos os dados em treino e teste e treinamos um Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Split treino/teste\n",
    "train, test = df_ml.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Modelo Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", probabilityCol=\"probability\", predictionCol=\"prediction\")\n",
    "model = rf.fit(train)\n",
    "\n",
    "# Predições\n",
    "predictions = model.transform(test)\n",
    "predictions.select(\"id_cliente\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Avaliação do Modelo\n",
    "Calculamos Acurácia e AUC-ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acurácia\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "\n",
    "# AUC-ROC\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "\n",
    "print(f\"Acurácia: {accuracy}\")\n",
    "print(f\"AUC-ROC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saída Final (Gold Layer)\n",
    "Geramos o DataFrame final `churn_predictions_gold` com id_cliente, probabilidade de churn e predição final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Extrair probabilidade de churn (classe 1)\n",
    "extract_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "churn_predictions_gold = predictions.withColumn(\"prob_churn\", extract_prob(col(\"probability\"))) \\\n",
    "    .select(\"id_cliente\", \"prob_churn\", \"prediction\")\n",
    "\n",
    "churn_predictions_gold.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Conclusão\n",
    "Neste notebook, implementamos um pipeline completo de previsão de churn:\n",
    "- Ingestão da camada Silver\n",
    "- Transformação e feature engineering para camada Gold\n",
    "- Treinamento com Random Forest\n",
    "- Avaliação com Acurácia e AUC-ROC\n",
    "- Geração do DataFrame final `churn_predictions_gold`\n",
    "\n",
    "Este notebook está pronto para ser executado no Databricks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
