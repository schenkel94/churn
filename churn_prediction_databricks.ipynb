{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Churn Prediction Pipeline (Free Edition)\n",
    "Pipeline completo de previsão de churn em PySpark, adaptado para rodar na versão gratuita do Databricks.\n",
    "\n",
    "## Estrutura:\n",
    "- Ingestão (Silver Layer)\n",
    "- Transformação e Feature Engineering (Gold Layer)\n",
    "- Treinamento de Modelo (Random Forest)\n",
    "- Avaliação (Acurácia e AUC-ROC)\n",
    "- Saída Final (churn_predictions_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Ingestão de Dados (Silver Layer)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ChurnPredictionPipeline\").getOrCreate()\n",
    "\n",
    "silver_path = \"/Volumes/workspace/voc/churn/churn_silver_2025.csv\"\n",
    "df_silver = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(silver_path)\n",
    "df_silver.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Transformação e Feature Engineering (Gold Layer)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Criar feature taxa de uso por valor pago\n",
    "df_gold = df_silver.withColumn(\"taxa_uso_valor\", col(\"total_logs_app_30d\") / col(\"valor_mensalidade\"))\n",
    "\n",
    "# Converter coluna alvo\n",
    "df_gold = df_gold.withColumn(\"label\", col(\"churn\").cast(\"integer\"))\n",
    "\n",
    "# Codificação manual da categoria\n",
    "df_gold = df_gold.withColumn(\n",
    "    \"categoria_index\",\n",
    "    when(col(\"categoria_principal_voc\") == \"API\", 0)\n",
    "    .when(col(\"categoria_principal_voc\") == \"BUGS\", 1)\n",
    "    .when(col(\"categoria_principal_voc\") == \"SUPORTE\", 2)\n",
    "    .when(col(\"categoria_principal_voc\") == \"FINANCEIRO\", 3)\n",
    "    .otherwise(4)\n",
    ")\n",
    "df_gold.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Preparação dos Dados para Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\"valor_mensalidade\", \"total_logs_app_30d\", \"tickets_suporte_abertos\", \"score_sentimento_voc\", \"taxa_uso_valor\", \"categoria_index\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "df_ml = assembler.transform(df_gold)\n",
    "df_ml.select(\"id_cliente\", \"features\", \"label\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Treinamento do Modelo (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "train, test = df_ml.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", probabilityCol=\"probability\", predictionCol=\"prediction\")\n",
    "model = rf.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.select(\"id_cliente\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator_auc.evaluate(predictions)\n",
    "\n",
    "print(f\"Acurácia: {accuracy}\")\n",
    "print(f\"AUC-ROC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Saída Final (Gold Layer)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "extract_prob = udf(lambda v: float(v[1]), DoubleType())\n",
    "churn_predictions_gold = predictions.withColumn(\"prob_churn\", extract_prob(col(\"probability\"))) \\\n",
    "    .select(\"id_cliente\", \"prob_churn\", \"prediction\")\n",
    "\n",
    "churn_predictions_gold.show(10)"
   ]
  }
 ]
}
