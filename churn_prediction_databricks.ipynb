{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b28dcdd-704f-4621-9814-632e0e6051b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pipeline de Churn Prediction\n",
    "\n",
    "Este notebook simula a transição da camada **Silver** para a **Gold**, aplicando engenharia de atributos (Feature Engineering) e treinando um modelo de Machine Learning para prever o cancelamento de clientes (Churn). Foi otimizado para a infraestrutura do Databricks Free Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad36f40e-544c-4782-adff-3babbaee331c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Instalação e Importação de Bibliotecas\n",
    "O Databricks já possui o PySpark nativamente, mas vamos importar as funções necessárias para o nosso pipeline de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db643db6-69d9-49a5-8c9e-92a0c335d29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, datediff, current_date, to_date\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c41fb0-e906-4bfc-b8c9-d7f5f5c2b1b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Ingestão de Dados (Camada Silver)\n",
    "Nesta etapa, carregamos os dados já limpos e estruturados da nossa tabela (ou arquivo CSV) na camada Silver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b127f9bd-cfa5-4928-a7e1-eace4f4a642b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lendo o CSV mapeado no Unity Catalog / Volumes (Camada Silver)\n",
    "file_path = \"/Volumes/workspace/voc/churn/churn_silver_2025.csv\"\n",
    "\n",
    "df_silver = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "display(df_silver.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7eb10f-163a-40d3-bf9a-380ec8a11d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Tratamento e Feature Engineering (Silver -> Gold)\n",
    "Criei novas variáveis (features) que ajudarão o algoritmo a encontrar padrões.\n",
    "* **taxa_uso_valor**: Relação entre os logs de uso e a mensalidade paga.\n",
    "* **dias_desde_assinatura**: O tempo de vida (tenure) do cliente na base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1703e5f1-ec08-442f-9393-06f6e7f80a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_prep = df_silver.withColumn(\n",
    "    \"taxa_uso_valor\", \n",
    "    col(\"total_logs_app_30d\") / (col(\"valor_mensalidade\") + 0.01) # Evita divisão por zero\n",
    ").withColumn(\n",
    "    \"dias_desde_assinatura\", \n",
    "    datediff(current_date(), to_date(col(\"data_assinatura\")))\n",
    ").na.drop() # Tratamento básico de eventuais nulos\n",
    "\n",
    "display(df_gold_prep.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40df4d35-f6f3-490b-9813-676b8c96dc22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Tratamento manual (Bypass de StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4e0113-3f2a-4efb-a29c-d64784e1cc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Em vez de StringIndexer, usamos mapeamento direto via Spark SQL (Não bloqueado)\n",
    "# Isso transforma as categorias em números manualmente\n",
    "categorias = [row[0] for row in df_gold_prep.select(\"categoria_principal_voc\").distinct().collect()]\n",
    "df_gold_indexed = df_gold_prep\n",
    "for i, cat in enumerate(categorias):\n",
    "    df_gold_indexed = df_gold_indexed.withColumn(\n",
    "        \"categoria_indexada\", \n",
    "        when(col(\"categoria_principal_voc\") == cat, float(i)).otherwise(col(\"categoria_indexada\") if \"categoria_indexada\" in df_gold_indexed.columns else 0.0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b488a1-dddc-4d99-b430-ad8cbf6b235d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Modelagem (Usando Scikit-Learn para evitar o bloqueio do Py4J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01764b31-d433-4911-95bf-e8ba154f0f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Em clusters Shared, o RandomForest do pyspark.ml costuma ser bloqueado.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Convertendo para Pandas apenas para o treinamento do modelo\n",
    "pdf = df_gold_indexed.select(\n",
    "    \"valor_mensalidade\", \"total_logs_app_30d\", \"tickets_suporte_abertos\", \n",
    "    \"score_sentimento_voc\", \"taxa_uso_valor\", \"dias_desde_assinatura\", \n",
    "    \"categoria_indexada\", \"churn\", \"id_cliente\"\n",
    ").toPandas()\n",
    "\n",
    "# Definindo Features e Target\n",
    "X = pdf.drop(['churn', 'id_cliente'], axis=1)\n",
    "y = pdf['churn']\n",
    "\n",
    "# Split simples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinando o Random Forest (Sklearn não sofre restrição de 'whitelist' do Py4J)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c806794-8856-455a-9ad2-838b1bfc33d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Avaliação do Modelo e saída da tabela Gold\n",
    "Testamos a qualidade da nossa IA em dados não vistos previamente utilizando duas métricas de classificação robustas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7d78f2-911f-421d-94cb-2a3365427ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(f\" Modelo Treinado no Cluster Shared!\")\n",
    "print(f\" Acurácia: {acc:.4f} | AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "# Criando o DataFrame Final (Gold) de volta para Spark\n",
    "pdf_results = pd.DataFrame({\n",
    "    'id_cliente': pdf.loc[X_test.index, 'id_cliente'],\n",
    "    'probabilidade_churn': y_prob,\n",
    "    'previsao_final': y_pred\n",
    "})\n",
    "\n",
    "churn_predictions_gold = spark.createDataFrame(pdf_results)\n",
    "display(churn_predictions_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b537cf-7cf0-445b-a97e-eedf470488b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Conclusão\n",
    "O pipeline foi executado com sucesso! \n",
    "Os dados brutos (Silver) foram higienizados e transformados matematicamente (Feature Eng). Após serem indexados e vetorizados, passaram por um classificador de Floresta Aleatória que identificou o padrão de evasão. Agora possuímos um Dataframe na camada Gold enriquecido com a probabilidade de evasão, pronto para ser plugado em painéis ou fluxos de CRM de retenção."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "churn_prediction_databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}